{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting S&P 500 index Using Simple Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created by Samuel Edet\n",
    "Date: 24th May, 2017\n",
    "Project: Forecasting S&P 500 index Using Simple Recurrent Neural Network.\n",
    "Title: Data Science Workshop\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Lemma:}$\n",
    "\n",
    "Let $A \\subset \\mathbb{R}^{n}$ and $B \\subset \\mathbb{R}^{m}$ be open sets, $D_{A} \\subset A$ and $D_{B} \\subset B$ be compact sets and $\\mathbf{F}:A \\times B \\rightarrow \\mathbb{R}^{n}$ be a $C^{1}$-class vector valued function. Given an invariant dynamic system of the form;\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\dot{x}}(t) = \\mathbf{F}(\\mathbf{x}(t),\\mathbf{u}(t)), \\mathbf{x} \\in A, \\mathbf{u} \\in B, t \\in \\mathbb{R}, \n",
    "\\end{align}$$\n",
    "\n",
    "with initial state $\\mathbf{x}(0)\\in D_{A}$. Then for an arbitrary $\\epsilon > 0$, there exist an integer $N$ and a recurrent neural network of the form;\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\dot{z}}(t) = \\sigma(W_{1}, \\mathbf{z}(t), W_{2}, \\mathbf{u}(t)), \n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "such that for any bounded input $\\mathbf{u}: \\mathbb{R} \\rightarrow D_{B}$,\n",
    "\n",
    "$$\\mbox{max}_{t \\in [0,T]} \\|\\mathbf{x}-\\mathbf{y}\\| < \\epsilon, \\quad 0 < T < \\infty \\; \\mbox{holds}.$$\n",
    "\n",
    "$\\textbf{Proof:}$\n",
    "\n",
    "Check the paper on the Approximtion of Dynamical Systems by Continuous Time Recurrent Neural Networks by Xiao-Dong Li, John K. L. Ho, and Tommy W. S. Chow; (http://www.ee.cityu.edu.hk/~twschow/pubs/papers/74.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Note:}$ The stock market is a time-variant dynamic system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Theorem:}$\n",
    "\n",
    "Let $A \\subset \\mathbb{R}^{n}$ and $B \\subset \\mathbb{R}^{m}$ be open sets, $D_{A} \\subset A$ and $D_{B} \\subset B$ be compact sets, and $\\mathbf{F}:A \\times B \\rightarrow \\mathbb{R}^{n}$ be a $C^{1}$ class vector valued function. Given a time-variant dynamic system of the form;\n",
    "\n",
    "$$\\begin{align}\n",
    "\\dot{\\mathbf{x}}(t) = \\mathbf{F}(\\mathbf{x}(t), \\mathbf{u}(t), t), \\quad \\mathbf{x} \\in A, \\mathbf{u} \\in B, t \\in \\mathbb{R}, \n",
    "\\end{align}$$\n",
    "\n",
    "with initial state $\\mathbf{x}(0)\\in D_{A}$. Then for an arbitrary $\\epsilon > 0$, there exist an integer $N$ and a recurrent neural network of the form (Lemma above), such that for any bounded input $\\mathbf{u}: \\mathbb{R} \\rightarrow D_{B}$,\n",
    "$$\\mbox{max}_{t \\in [0,T]} \\|\\mathbf{x-y}\\| < \\epsilon, \\quad 0 < T < \\infty \\; \\mbox{holds}.$$\n",
    "\n",
    "\n",
    "$\\textbf{Proof:}$\n",
    "\n",
    "Let $\\mathbf{\\overline{x}}(t) = \\begin{pmatrix}\n",
    "\\mathbf{x}(t) \\\\ t \\end{pmatrix} \\in \\mathbb{R}^{n+1}$. We can write the time variant system (in the theorem) as a time invariant system (in the lemma);\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\dot{\\overline{x}}}(t) = \\mathbf{\\overline{F}}(\\mathbf{\\overline{x}}(t), \\mathbf{u}(t)), \\quad \\mathbf{\\overline{x}} \\in A \\times \\mathbb{R}, \\mathbf{u} \\in B, \n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mathbf{\\overline{F}}(\\mathbf{\\overline{x}}(t), \\mathbf{u}(t)) = \\begin{pmatrix}\n",
    "\\mathbf{F}(\\mathbf{x}(t), \\mathbf{u}(t),\\mathbf{\\overline{x}}_{n+1}(t)) \\\\ 1 \\end{pmatrix}.$ Since $\\mathbf{\\overline{F}}_{n+1} = 1$ is first order continuously differentiable. Therefore $\\mathbf{\\overline{F}}$ is a $C^{1}$-class vector valued function.\n",
    "\n",
    "The initial condition of the dynamical system (in the theorem) is given as $\\mathbf{\\overline{x}}(0) = \\begin{pmatrix}\n",
    "\\mathbf{x}(0) \\\\ 0 \\end{pmatrix}$. From the theorem, we have that $\\mathbf{x}(0) \\in D_{A}$, where $D_{A} \\subset A$ is a  compact set. Therefore $\\mathbf{\\overline{x}}(0) \\in D_{A} \\times [0,T]$, where $[0,T] \\subset \\mathbb{R}$ is compact. Since the cartesian product of compact set is compact, we have that $ D_{A} \\times [0,T]$ is a compact subset of $A \\times \\mathbb{R}$.\n",
    "\n",
    "Therefore, we have that the time invariant system satisfies the Lemma. Hence, \n",
    "\n",
    "$$\\begin{align}\n",
    "\\mbox{max}_{t \\in [0,T]} \\|\\mathbf{\\overline{x}}(t) - \\mathbf{\\overline{y}}(t)\\| < \\epsilon .\n",
    "\\end{align}$$\n",
    "\n",
    "The neural state is given as $\\mathbf{z} = (\\mathbf{\\overline{y}},\\mathbf{\\overline{h}}) \\in \\mathbb{R}^{n+N}$, where $\\mathbf{\\overline{y}} \\in \\mathbb{R}^{n+1} \\mbox{and} \\; \\mathbf{\\overline{h}} \\in \\mathbb{R}^{N-1}$. Since $\\mathbf{\\overline{y}} = (\\mathbf{y}, \\mathbf{\\overline{y}}_{n+1}) \\in \\mathbb{R}^{n+1}$, then $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the neural output state for the first $n$ units. The Euclidean distance between $\\mathbf{\\overline{x}}$ and $\\mathbf{\\overline{y}}$ is given as;\n",
    "\n",
    "$$\\begin{align}\n",
    "\\| \\mathbf{\\overline{x}} - \\mathbf{\\overline{y}} \\|^{2} = \\| \\mathbf{x} - \\mathbf{y} \\|^{2} + (\\mathbf{\\overline{x}}_{n+1} - \\mathbf{\\overline{y}}_{n+1})^{2} . \n",
    "\\end{align}$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mbox{max}_{t \\in [0,T]} \\| \\mathbf{x}(t) - \\mathbf{y}(t) \\| \\leq \\mbox{max}_{t \\in [0,T]} \\| \\mathbf{\\overline{x}}(t) - \\mathbf{\\overline{y}}(t) \\| < \\epsilon .\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "#import pandas.io.data as web\n",
    "from pandas_datareader import data as web\n",
    "import math\n",
    "\n",
    "\n",
    "import theano\n",
    "import theano.tensor as TT\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation,GRU, SimpleRNN\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,explained_variance_score,mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uploading the csv file\n",
    "features = pd.DataFrame.from_csv('features.csv')\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Log returns of the dataset\n",
    "features= np.log(features / features.shift(1))\n",
    "features = features.fillna(np.mean(features.ix[:]))\n",
    "features = features.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation of datassets\n",
    "features.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From the correlation table the selected features are \n",
    "#SPYt-1, JNJ, IXIC and the different levels of neurons and epoch.\n",
    "X = features.ix[:,['SPYt-1','JNJ','^IXIC']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To determine the upward or downward movement the log return of the features\n",
    "X[1:,:]=X[1:,:]-X[0:-1,:]   #differnce between today's return and yesterday's return for the training data set\n",
    "X[0,:]=0.000001\n",
    "X_del_1=X[0:-1,:]  #exludes the last row  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the target stock is SPY\n",
    "y = features.ix[:,'SPY'].values\n",
    "\n",
    "#To determine the upward or downward movement the log return of the target\n",
    "y[1:]=y[1:]-y[0:-1] \n",
    "y[0]=.000001\n",
    "\n",
    "y = np.where(y >= 0.0, 1, 0)\n",
    "y_del_1=y[1:]      #exludes the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allocating dataset into training and testing\n",
    "#80% is allocated to training and 20% to testing\n",
    "#allocation for X,y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.2, random_state=0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standardising training and testing dataset\n",
    "sc = StandardScaler()\n",
    "#standardising  for X\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "X_train_std = np.reshape(X_train_std, (X_train_std.shape[0], X_train_std.shape[1], 1))\n",
    "X_test_std = np.reshape(X_test_std, (X_test_std.shape[0], X_test_std.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "#THE NEURAL NETWORK MODELS\n",
    "\n",
    "#Simple RNN model\n",
    "start_time = time.time() #timing the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(75,input_dim=1))  #75 hidden neurons\n",
    "model.add(Dropout(0.2))   #to reduce overfitting\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit the model\n",
    "model.fit(X_train_std, y_train, epochs=5, batch_size=1)\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time was %g seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicting the movement of the next 99 days of SPY\n",
    "y_pred_model = model.predict(X_test_std)[1:100]\n",
    "y_pred_model[1:]=y_pred_model[1:]-y_pred_model[0:-1] \n",
    "y_pred_model[0]=.000001\n",
    "y_pred_model = np.where(y_pred_model >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation Report\n",
    "print('Misclassified samples: %d' % (y_test[1:100] != y_pred_model[:,0]).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test[1:100], y_pred_model))\n",
    "print(confusion_matrix(y_test[1:100],y_pred_model))\n",
    "print(classification_report(y_test[1:100],y_pred_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For LSTM \n",
    "#Simply change the SimpleRNN in the code above to LSTM.\n",
    "\n",
    "#GRU models,\n",
    "#Simply change the SimpleRNN in the code above to GRU.\n",
    "\n",
    "\n",
    "#------------------------------------------\n",
    "#After performing the 20 experiments for each \n",
    "#of SimpleRNN, LSTM and GRU\n",
    "\n",
    "#plot for accuracy of experiments\n",
    "experiments = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "x = np.array(experiments)\n",
    "\n",
    "y = [0.73,0.73,0.70,0.70,0.70,0.74,0.72,0.72,0.71,0.72,\n",
    "     0.75,0.73,0.74,0.74,0.72,0.74,0.72,0.74,0.73,0.75]\n",
    "z = [0.73,0.74,0.73,0.73,0.71,0.74,0.73,0.71,0.69,0.70,\n",
    "    0.71,0.71,0.69,0.70,0.71,0.73,0.71,0.69,0.69,0.66]\n",
    "k = [0.71,0.73,0.72,0.71,0.71,0.74,0.70,0.72,0.72,0.70,\n",
    "    0.73,0.71,0.68,0.70,0.70,0.73,0.69,0.69,0.69,0.69]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(x-0.25, y,width=0.25,color='b',align='center',label='Simple RNN')\n",
    "ax.bar(x, z,width=0.25,color='g',align='center',label='LSTM')\n",
    "ax.bar(x+0.25, k,width=0.25,color='r',align='center',label ='GRU')\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylim(0.6,0.8)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Experiments')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xticks(experiments)\n",
    "plt.savefig('myfig1',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot for GPU time of experiments\n",
    "y1 = [50.566,97.8223,144.874,187.599,233.552,53.3097,102.235,\n",
    "     150.565,198.614,247.727,58.6347,177.455,166.386,218.544,\n",
    "     271.841,63.135,120.761,180.189,237.193,373.436]  #srnn \n",
    "\n",
    "z1 = [445.469,880.36,1289.69,1722.03,2692.28,455.511,892.438,\n",
    "      1328.87,1768.11,2812.48,456.057,889.301,1336.44,\n",
    "      1812.23,2798.16,483.806,940.876,1409.29,1912.45,2886.09] #lstm\n",
    "\n",
    "k1 = [449.948,779.411,1146.83,1555.16,1792.24,409.454,810.361,\n",
    "      1211.28,1588.98,1812.24,411.027,808.906,1195.79,1584.82,\n",
    "      1884.32,406.104,806.767,1211.98,1601.12,2006.17] #gru\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(x-0.25, y1,width=0.25,color='b',align='center',label='Simple RNN')\n",
    "ax.bar(x, z1,width=0.25,color='g',align='center',label='LSTM')\n",
    "ax.bar(x+0.25, k1,width=0.25,color='r',align='center',label='GRU')\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('Time(seconds)')\n",
    "plt.title('GPU time of Experiments')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xticks(experiments)\n",
    "plt.savefig('myfig',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------\n",
    "#Simulating profit\n",
    "'''\n",
    "Mov = predicted movement of stock\n",
    "Inc = Initial capital\n",
    "Stok = Stock, which is a form of capital\n",
    "Cash = another form of capital\n",
    "Cp = closing price\n",
    "'''\n",
    "def simul_profit(Inc,Cash,Stock,Mov):\n",
    "    Cash, Stock = Inc, 0\n",
    "    for i in range(len(Mov)):\n",
    "        if Mov[i]==1:\n",
    "            if Stock==0:\n",
    "                s=Cash/Cp[i]\n",
    "                Stock=int(s)\n",
    "                Cash=(s-Stock)*Cp[i]\n",
    "        else:\n",
    "            if Stock>0:\n",
    "                Cash +=Stock*Cp[i]\n",
    "                Stock=0\n",
    "    Cash+=Stock*Cp[-1]\n",
    "    print(Cash-Inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example of how the function works\n",
    "Mov = [1,1,1,0,0]\n",
    "Cp = [201, 202.56, 197, 202,199]\n",
    "simul_profit(1000,1000,0,Mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image2.png\" > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.png\" > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
